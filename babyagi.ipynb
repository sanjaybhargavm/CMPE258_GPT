{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "556af556",
      "metadata": {
        "id": "556af556"
      },
      "source": [
        "**libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai tiktoken faiss-cpu google-search-results  -q"
      ],
      "metadata": {
        "id": "SGKGWyzGdymh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372231a9-85ca-4060-8ff5-d91b299063cc"
      },
      "id": "SGKGWyzGdymh",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.6/727.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c8a354b6",
      "metadata": {
        "id": "c8a354b6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import deque\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "from langchain import LLMChain, OpenAI, PromptTemplate\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.llms import BaseLLM\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.chains.base import Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "openapi key"
      ],
      "metadata": {
        "id": "HOeYKwpi6-W1"
      },
      "id": "HOeYKwpi6-W1"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-LgjVZMMJefX7fucws9CXT3BlbkFJIm22bmfsdvfvbIwvEMVuYoUL'\n",
        "os.environ['SERPAPI_API_KEY'] = '58167fd162a5ec71e5e6e1b779c4a5f0176635f53e283a6ebdff43f6b6783c33620'"
      ],
      "metadata": {
        "id": "dUnHiZbMeRJH"
      },
      "id": "dUnHiZbMeRJH",
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "09f70772",
      "metadata": {
        "id": "09f70772"
      },
      "source": [
        "Using FAISS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "794045d4",
      "metadata": {
        "id": "794045d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0210f2-e04b-4b4b-ef0a-ecc142321af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "%pip install faiss-cpu google-search-results "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore import InMemoryDocstore\n",
        "from langchain.vectorstores.faiss import FAISS"
      ],
      "metadata": {
        "id": "U40MjY8Ue16_"
      },
      "id": "U40MjY8Ue16_",
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Model"
      ],
      "metadata": {
        "id": "UzFPR-bq71VP"
      },
      "id": "UzFPR-bq71VP"
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "6e0305eb",
      "metadata": {
        "id": "6e0305eb"
      },
      "outputs": [],
      "source": [
        "embeddings_model = OpenAIEmbeddings()\n",
        "\n",
        "import faiss\n",
        "embedding_size = 1536\n",
        "index = faiss.IndexFlatL2(embedding_size)\n",
        "vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the Creation chain, Prioritization chain, and Execution chain(agent)"
      ],
      "metadata": {
        "id": "mMehyBPj8pQq"
      },
      "id": "mMehyBPj8pQq"
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "bf4bd5cd",
      "metadata": {
        "id": "bf4bd5cd"
      },
      "outputs": [],
      "source": [
        "class TaskCreationChain(LLMChain):\n",
        "    \"\"\"Chain to generates tasks.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
        "        \"\"\"Get the response parser.\"\"\"\n",
        "        task_creation_template = (\n",
        "            \"You are an task creation AI that uses the result of an execution agent\"\n",
        "            \" to create new tasks with the following objective: {objective},\"\n",
        "            \" The last completed task has the result: {result}.\"\n",
        "            \" This result was based on this task description: {task_description}.\"\n",
        "            \" These are incomplete tasks: {incomplete_tasks}.\"\n",
        "            \" Based on the result, create new tasks to be completed\"\n",
        "            \" by the AI system that do not overlap with incomplete tasks.\"\n",
        "            \" Return the tasks as an array.\"\n",
        "        )\n",
        "        prompt = PromptTemplate(\n",
        "            template=task_creation_template,\n",
        "            input_variables=[\"result\", \"task_description\", \"incomplete_tasks\", \"objective\"],\n",
        "        )\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "b6488ffe",
      "metadata": {
        "id": "b6488ffe"
      },
      "outputs": [],
      "source": [
        "class TaskPrioritizationChain(LLMChain):\n",
        "    \"\"\"Chain to prioritize tasks.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(cls, llm: BaseLLM, verbose: bool = True) -> LLMChain:\n",
        "        \"\"\"Get the response parser.\"\"\"\n",
        "        task_prioritization_template = (\n",
        "            \"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing\"\n",
        "            \" the following tasks: {task_names}.\"\n",
        "            \" Consider the ultimate objective of your team: {objective}.\"\n",
        "            \" Do not remove any tasks. Return the result as a numbered list, like:\"\n",
        "            \" #. First task\"\n",
        "            \" #. Second task\"\n",
        "            \" Start the task list with number {next_task_id}.\"\n",
        "        )\n",
        "        prompt = PromptTemplate(\n",
        "            template=task_prioritization_template,\n",
        "            input_variables=[\"task_names\", \"next_task_id\", \"objective\"],\n",
        "        )\n",
        "        return cls(prompt=prompt, llm=llm, verbose=verbose)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "b43cd580",
      "metadata": {
        "id": "b43cd580"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
        "from langchain import OpenAI, SerpAPIWrapper, LLMChain\n",
        "todo_prompt = PromptTemplate.from_template(\"You are a planner who is an expert at coming up with a todo list for a given objective. Come up with a todo list for this objective: {objective}\")\n",
        "todo_chain = LLMChain(llm=OpenAI(temperature=0), prompt=todo_prompt)\n",
        "search = SerpAPIWrapper()\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to answer questions about current events\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"TODO\",\n",
        "        func=todo_chain.run,\n",
        "        description=\"useful for when you need to come up with todo lists. Input: an objective to create a todo list for. Output: a todo list for that objective. Please be very clear what the objective is!\"\n",
        "    )\n",
        "]\n",
        "\n",
        "\n",
        "prefix = \"\"\"You are an AI who performs one task based on the following objective: {objective}. Take into account these previously completed tasks: {context}.\"\"\"\n",
        "suffix = \"\"\"Question: {task}\n",
        "{agent_scratchpad}\"\"\"\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools, \n",
        "    prefix=prefix, \n",
        "    suffix=suffix, \n",
        "    input_variables=[\"objective\", \"task\", \"context\",\"agent_scratchpad\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad996c5",
      "metadata": {
        "id": "3ad996c5"
      },
      "source": [
        "###Defining the BabyAGI Controller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "0ada0636",
      "metadata": {
        "id": "0ada0636"
      },
      "outputs": [],
      "source": [
        "def get_next_task(task_creation_chain: LLMChain, result: Dict, task_description: str, task_list: List[str], objective: str) -> List[Dict]:\n",
        "    \"\"\"Get the next task.\"\"\"\n",
        "    incomplete_tasks = \", \".join(task_list)\n",
        "    response = task_creation_chain.run(result=result, task_description=task_description, incomplete_tasks=incomplete_tasks, objective=objective)\n",
        "    new_tasks = response.split('\\n')\n",
        "    return [{\"task_name\": task_name} for task_name in new_tasks if task_name.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "d35250ad",
      "metadata": {
        "id": "d35250ad"
      },
      "outputs": [],
      "source": [
        "def prioritize_tasks(task_prioritization_chain: LLMChain, this_task_id: int, task_list: List[Dict], objective: str) -> List[Dict]:\n",
        "    \"\"\"Prioritize tasks.\"\"\"\n",
        "    task_names = [t[\"task_name\"] for t in task_list]\n",
        "    next_task_id = int(this_task_id) + 1\n",
        "    response = task_prioritization_chain.run(task_names=task_names, next_task_id=next_task_id, objective=objective)\n",
        "    new_tasks = response.split('\\n')\n",
        "    prioritized_task_list = []\n",
        "    for task_string in new_tasks:\n",
        "        if not task_string.strip():\n",
        "            continue\n",
        "        task_parts = task_string.strip().split(\".\", 1)\n",
        "        if len(task_parts) == 2:\n",
        "            task_id = task_parts[0].strip()\n",
        "            task_name = task_parts[1].strip()\n",
        "            prioritized_task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n",
        "    return prioritized_task_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "e3f1840c",
      "metadata": {
        "id": "e3f1840c"
      },
      "outputs": [],
      "source": [
        "def _get_top_tasks(vectorstore, query: str, k: int) -> List[str]:\n",
        "    \"\"\"Get the top k tasks based on the query.\"\"\"\n",
        "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
        "    if not results:\n",
        "        return []\n",
        "    sorted_results, _ = zip(*sorted(results, key=lambda x: x[1], reverse=True))\n",
        "    return [str(item.metadata['task']) for item in sorted_results]\n",
        "\n",
        "def execute_task(vectorstore, execution_chain: LLMChain, objective: str, task: str, k: int = 5) -> str:\n",
        "    \"\"\"Execute a task.\"\"\"\n",
        "    context = _get_top_tasks(vectorstore, query=objective, k=k)\n",
        "    return execution_chain.run(objective=objective, context=context, task=task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "1e978938",
      "metadata": {
        "id": "1e978938"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BabyAGI(Chain, BaseModel):\n",
        "    \"\"\"Controller model for the BabyAGI agent.\"\"\"\n",
        "\n",
        "    task_list: deque = Field(default_factory=deque)\n",
        "    task_creation_chain: TaskCreationChain = Field(...)\n",
        "    task_prioritization_chain: TaskPrioritizationChain = Field(...)\n",
        "    execution_chain: AgentExecutor = Field(...)\n",
        "    task_id_counter: int = Field(1)\n",
        "    vectorstore: VectorStore = Field(init=False)\n",
        "    max_iterations: Optional[int] = None\n",
        "        \n",
        "    class Config:\n",
        "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def add_task(self, task: Dict):\n",
        "        self.task_list.append(task)\n",
        "\n",
        "    def print_task_list(self):\n",
        "        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        for t in self.task_list:\n",
        "            print(str(t[\"task_id\"]) + \": \" + t[\"task_name\"])\n",
        "\n",
        "    def print_next_task(self, task: Dict):\n",
        "        print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        print(str(task[\"task_id\"]) + \": \" + task[\"task_name\"])\n",
        "\n",
        "    def print_task_result(self, result: str):\n",
        "        print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "        print(result)\n",
        "        \n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        return [\"objective\"]\n",
        "    \n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return []\n",
        "\n",
        "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Run the agent.\"\"\"\n",
        "        objective = inputs['objective']\n",
        "        first_task = inputs.get(\"first_task\", \"Make a todo list\")\n",
        "        self.add_task({\"task_id\": 1, \"task_name\": first_task})\n",
        "        num_iters = 0\n",
        "        while True:\n",
        "            if self.task_list:\n",
        "                self.print_task_list()\n",
        "\n",
        "                # Step 1: Pull the first task\n",
        "                task = self.task_list.popleft()\n",
        "                self.print_next_task(task)\n",
        "\n",
        "                # Step 2: Execute the task\n",
        "                result = execute_task(\n",
        "                    self.vectorstore, self.execution_chain, objective, task[\"task_name\"]\n",
        "                )\n",
        "                this_task_id = int(task[\"task_id\"])\n",
        "                self.print_task_result(result)\n",
        "\n",
        "                # Step 3: Store the result in Pinecone\n",
        "                result_id = f\"result_{task['task_id']}\"\n",
        "                self.vectorstore.add_texts(\n",
        "                    texts=[result],\n",
        "                    metadatas=[{\"task\": task[\"task_name\"]}],\n",
        "                    ids=[result_id],\n",
        "                )\n",
        "\n",
        "                # Step 4: Create new tasks and reprioritize task list\n",
        "                new_tasks = get_next_task(\n",
        "                    self.task_creation_chain, result, task[\"task_name\"], [t[\"task_name\"] for t in self.task_list], objective\n",
        "                )\n",
        "                for new_task in new_tasks:\n",
        "                    self.task_id_counter += 1\n",
        "                    new_task.update({\"task_id\": self.task_id_counter})\n",
        "                    self.add_task(new_task)\n",
        "                self.task_list = deque(\n",
        "                    prioritize_tasks(\n",
        "                        self.task_prioritization_chain, this_task_id, list(self.task_list), objective\n",
        "                    )\n",
        "                )\n",
        "            num_iters += 1\n",
        "            if self.max_iterations is not None and num_iters == self.max_iterations:\n",
        "                print(\"\\033[91m\\033[1m\" + \"\\n*****TASK ENDING*****\\n\" + \"\\033[0m\\033[0m\")\n",
        "                break\n",
        "        return {}\n",
        "\n",
        "    @classmethod\n",
        "    def from_llm(\n",
        "        cls,\n",
        "        llm: BaseLLM,\n",
        "        vectorstore: VectorStore,\n",
        "        verbose: bool = False,\n",
        "        **kwargs\n",
        "    ) -> \"BabyAGI\":\n",
        "        \"\"\"Initialize the BabyAGI Controller.\"\"\"\n",
        "        task_creation_chain = TaskCreationChain.from_llm(\n",
        "            llm, verbose=verbose\n",
        "        )\n",
        "        task_prioritization_chain = TaskPrioritizationChain.from_llm(\n",
        "            llm, verbose=verbose\n",
        "        )\n",
        "        llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "        tool_names = [tool.name for tool in tools]\n",
        "        agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\n",
        "        agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
        "        return cls(\n",
        "            task_creation_chain=task_creation_chain,\n",
        "            task_prioritization_chain=task_prioritization_chain,\n",
        "            execution_chain=agent_executor,\n",
        "            vectorstore=vectorstore,\n",
        "            **kwargs\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "usig babyagi to create tasks"
      ],
      "metadata": {
        "id": "Z6SbBVldVoIJ"
      },
      "id": "Z6SbBVldVoIJ"
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "3d220b69",
      "metadata": {
        "id": "3d220b69"
      },
      "outputs": [],
      "source": [
        "OBJECTIVE = \"Report about current trends in tech\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "8a8e5543",
      "metadata": {
        "id": "8a8e5543"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "3d69899b",
      "metadata": {
        "id": "3d69899b"
      },
      "outputs": [],
      "source": [
        "# Logging of LLMChains\n",
        "verbose=False\n",
        "# If None, will keep on going forever\n",
        "max_iterations: Optional[int] = 1\n",
        "baby_agi = BabyAGI.from_llm(\n",
        "    llm=llm,\n",
        "    vectorstore=vectorstore,\n",
        "    verbose=verbose,\n",
        "    max_iterations=max_iterations\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handing over task to baby_agi and getting results"
      ],
      "metadata": {
        "id": "oyvkh4Id_qUN"
      },
      "id": "oyvkh4Id_qUN"
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "f7957b51",
      "metadata": {
        "scrolled": false,
        "id": "f7957b51",
        "outputId": "f4cde219-6d11-4e5d-9a8e-72f32f47e278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95m\u001b[1m\n",
            "*****TASK LIST*****\n",
            "\u001b[0m\u001b[0m\n",
            "1: Make a todo list\n",
            "\u001b[92m\u001b[1m\n",
            "*****NEXT TASK*****\n",
            "\u001b[0m\u001b[0m\n",
            "1: Make a todo list\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: What tasks should I complete to make a todo list?\n",
            "Action: TODO\n",
            "Action Input: Create a todo list for current trends in tech\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m\n",
            "\n",
            "1. Research current trends in tech\n",
            "2. Identify the most relevant trends\n",
            "3. Analyze the impact of the trends on the tech industry\n",
            "4. Create a list of tasks related to the trends\n",
            "5. Prioritize the tasks based on importance\n",
            "6. Develop a timeline for completing the tasks\n",
            "7. Create a budget for the tasks\n",
            "8. Identify resources needed to complete the tasks\n",
            "9. Monitor progress and adjust the timeline and budget as needed\n",
            "10. Evaluate the success of the todo list\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The final answer is a todo list for current trends in tech that includes researching current trends, identifying the most relevant trends, analyzing the impact of the trends on the tech industry, creating a list of tasks related to the trends, prioritizing the tasks based on importance, developing a timeline for completing the tasks, creating a budget for the tasks, identifying resources needed to complete the tasks, monitoring progress and adjusting the timeline and budget as needed, and evaluating the success of the todo list.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[93m\u001b[1m\n",
            "*****TASK RESULT*****\n",
            "\u001b[0m\u001b[0m\n",
            "The final answer is a todo list for current trends in tech that includes researching current trends, identifying the most relevant trends, analyzing the impact of the trends on the tech industry, creating a list of tasks related to the trends, prioritizing the tasks based on importance, developing a timeline for completing the tasks, creating a budget for the tasks, identifying resources needed to complete the tasks, monitoring progress and adjusting the timeline and budget as needed, and evaluating the success of the todo list.\n",
            "\u001b[91m\u001b[1m\n",
            "*****TASK ENDING*****\n",
            "\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'objective': 'Report about current trends in tech'}"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "baby_agi({\"objective\": OBJECTIVE})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "id": "898a210b",
      "metadata": {
        "id": "898a210b"
      },
      "outputs": [],
      "source": [
        "OBJECTIVE = \"Report about current seasonal situation in san jose\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "Zm8aE0Q6WKF-"
      },
      "id": "Zm8aE0Q6WKF-",
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logging of LLMChains\n",
        "verbose=False\n",
        "# If None, will keep on going forever\n",
        "max_iterations: Optional[int] = 1\n",
        "baby_agi = BabyAGI.from_llm(\n",
        "    llm=llm,\n",
        "    vectorstore=vectorstore,\n",
        "    verbose=verbose,\n",
        "    max_iterations=max_iterations\n",
        ")"
      ],
      "metadata": {
        "id": "WHhnpjKPWKCe"
      },
      "id": "WHhnpjKPWKCe",
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handing over task to baby_agi and getting results"
      ],
      "metadata": {
        "id": "3-PgfmUJAIXC"
      },
      "id": "3-PgfmUJAIXC"
    },
    {
      "cell_type": "code",
      "source": [
        "baby_agi({\"objective\": OBJECTIVE})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAYComtEWJ-1",
        "outputId": "a36bea4d-a7b2-416e-c762-cac123048970"
      },
      "id": "nAYComtEWJ-1",
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[95m\u001b[1m\n",
            "*****TASK LIST*****\n",
            "\u001b[0m\u001b[0m\n",
            "1: Make a todo list\n",
            "\u001b[92m\u001b[1m\n",
            "*****NEXT TASK*****\n",
            "\u001b[0m\u001b[0m\n",
            "1: Make a todo list\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: What tasks should I include in the todo list?\n",
            "Action: TODO\n",
            "Action Input: Report about current seasonal situation in san jose\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3m\n",
            "\n",
            "1. Research current seasonal weather patterns in San Jose\n",
            "2. Gather data on average temperatures, precipitation, and other weather-related information\n",
            "3. Analyze data to identify any trends or patterns\n",
            "4. Research current seasonal events in San Jose\n",
            "5. Gather information on festivals, concerts, and other seasonal activities\n",
            "6. Research current seasonal attractions in San Jose\n",
            "7. Gather information on parks, museums, and other attractions\n",
            "8. Compile data and create report\n",
            "9. Write report summarizing current seasonal situation in San Jose\n",
            "10. Proofread and edit report\n",
            "11. Submit report\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The final todo list for reporting about the current seasonal situation in San Jose is: \n",
            "1. Research current seasonal weather patterns in San Jose\n",
            "2. Gather data on average temperatures, precipitation, and other weather-related information\n",
            "3. Analyze data to identify any trends or patterns\n",
            "4. Research current seasonal events in San Jose\n",
            "5. Gather information on festivals, concerts, and other seasonal activities\n",
            "6. Research current seasonal attractions in San Jose\n",
            "7. Gather information on parks, museums, and other attractions\n",
            "8. Compile data and create report\n",
            "9. Write report summarizing current seasonal situation in San Jose\n",
            "10. Proofread and edit report\n",
            "11. Submit report\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[93m\u001b[1m\n",
            "*****TASK RESULT*****\n",
            "\u001b[0m\u001b[0m\n",
            "The final todo list for reporting about the current seasonal situation in San Jose is: \n",
            "1. Research current seasonal weather patterns in San Jose\n",
            "2. Gather data on average temperatures, precipitation, and other weather-related information\n",
            "3. Analyze data to identify any trends or patterns\n",
            "4. Research current seasonal events in San Jose\n",
            "5. Gather information on festivals, concerts, and other seasonal activities\n",
            "6. Research current seasonal attractions in San Jose\n",
            "7. Gather information on parks, museums, and other attractions\n",
            "8. Compile data and create report\n",
            "9. Write report summarizing current seasonal situation in San Jose\n",
            "10. Proofread and edit report\n",
            "11. Submit report\n",
            "\u001b[91m\u001b[1m\n",
            "*****TASK ENDING*****\n",
            "\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'objective': 'Report about current seasonal situation in san jose'}"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}